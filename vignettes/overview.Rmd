---
title: "Simulation methods"
output: 
    rmarkdown::pdf_document:
        citation_package: natbib
        keep_tex: true
bibliography: pobib.bib
vignette: >
  %\VignetteIndexEntry{Simulation methods}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, include = FALSE}
#library(pseudoglm)
```

# Introduction

Let $T$ denote the time to event, and $\Delta \in \{1, \ldots, d\}$ denote the indicator of the cause of the event for $d$ competing causes. In analyses of these sort of data, it may be of interest to estimate and model the cause specific cumulative incidence of cause $1$ (WOLOG) at a particular time $t^*$: 
\[
P(T < t^*, \Delta = 1)
\]
or the restricted mean life years lost due to cause $1$ up to time $t^*$: 
\[
\int_0^{t^*} P(T < u, \Delta = 1) \, du.
\]

For an overview of the methods, see @andersen_cause-specific_2013.

# Simulation study

We conducted a simulation study with the goal of determining which methods should be used as the defaults in our `eventglm` package. The key criteria are validity, as measured by type I error rates, bias, and confidence interval coverage, robustness to misspecification of the censoring mechanism, and statistical efficiency. A lesser concern is computational efficiency. 

We generated datasets with competing risks according to @beyersmann09 as follows: 
We first generated a binary covariate $Z$ as Bernoulli with probability 0.5, and two independent standard normal variables $X_1, X_2$. Then $\mathbf{Q} = (1, Z, X_1, X_2)$. 
We used a proportional hazards Weibull distribution to generate the time data for $k = 1, 2$, with a hazard of: $h_k(t|\mathbf{Q})= \gamma_k*(1/e^{(\mathbf{Q}^T{\zeta}_k)})^{\gamma_k}*t^{\gamma_k-1}$
and a cumulative hazard given by:
$H_k(t|\mathbf{Q})= (1/e^{(\mathbf{Q}^T{\zeta}_k)})^{\gamma_k}*(t)^{\gamma_k},$
where $\mathbf{Q}$ is the vector of all covariates of interest in this order $(1, Z, X1,X2)$, which then correspond to the cause specific vector of coefficients ${\zeta}_k=(\zeta_0,\zeta_z, \zeta_{x1}, \zeta_{x2})$.
The overall survivor function is then given by:
$Surv(t|\mathbf{Q}) = \mbox{Exp}{(-\sum_k H_k(t|\mathbf{Q}))}.$

We create overall survival times by inverting the CDF, one less the survivor, using the probability integral transform to obtain overall survival times, $Tov$. We then determine which of the event types a time belongs to by randomly generating from a Bernoulli with probability $h_m(Tov|\mathbf{Q})/(h_m(Tov|\mathbf{Q})+h_{m'}(Tov|\mathbf{Q}))$ and assigning event type 1 if 1 and 2 if 0.  We then generate censoring times using `rweibull` with shape parameter equal to $e^{\mathbf{Q}^T \alpha}$ and scale parameter $\gamma_c$. The intercept (i.e., first element) of $\alpha$ determines the amount of censoring, and whether the remaining coefficients are non-zero determines whether the censoring depends on covariates. When $\gamma_c = 1$, the censoring times follow a proportional hazards model, and thus the Cox model for the censoring times is correctly specified. 

The true values of the coefficients were determined by generated a very large sample of covariates $\mathbf{Q}$, then calculating the corresponding true values of the cumulative incidence or restricted mean life time lost, and finally regressing those true values against the covariates using the link function. Samples large enough to acheive a precision of 1e-4 on the coefficient values were used. 


# References



